# -*- coding: utf-8 -*-
"""Trabalho 3 CD

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kwDBhZD05HCPfgCWO7F3KaUj_M5ZcoKY
"""

from google.colab import drive
drive.mount('/content/drive')

"""## Trabalho 3 de Ciência de Dados: Spam prediction

Observação: esse colab contém os principais passos realizados para obter o melhor classificador. No outro colab está o classificador em sua versão final.

Alunas:

*   Milena Lucas dos Santos
*   Raianny Proença de C. Oliveira
*   Vivian Miwa Fugihara

### Imports
"""

import pandas as pd
import numpy as np
from time import time
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn import datasets
from sklearn import svm
import matplotlib.pyplot as plt
from sklearn.model_selection import cross_validate
from google.colab import files
from sklearn.metrics import confusion_matrix
from sklearn.datasets import load_boston
from sklearn.feature_selection import SelectKBest,f_regression
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error
import imblearn

"""## Upload do conjunto de dados (será usado para o arquivo teste do prof)"""

# faz upload de arquivo da base de dados
uploaded = files.upload()

uploaded # amostra o arquivo lido separado por virgulas

# joga pra um dataframe
uploaded_emails = pd.read_csv('emails.csv', sep=',')
uploaded_emails

"""# Lendo o conjunto de dados do drive"""

dataset = pd.read_csv('/content/drive/MyDrive/Trabalho3CD/emails.csv')
#dataset = pd.read_csv('emails.csv')

dataset

"""## Manipulando o dataset"""

# remover coluna com número do email, pois não é um atributo preditivo relevante por ter valores únicos.
dataf = dataset.drop(["Email No."], axis=1)

# mostrar linhas (qdte emails) e colunas (qtde atributos)
dataf.shape

dataframe = pd.DataFrame(dataf.drop(["Prediction"], axis=1))

"""# Normalizando os dados

O algoritmo KNN é definido em termos de distância, sendo assim realizar a normalização ou padronização dos dados é muito importante. Há duas métricas muito conhecidas que são: a normalização MinMax e Standard Scaler. A métrica MinMax normaliza os dados entre 0 e 1. O Standard Scaler mapeia o dado em uma distribuição cuja média é definida como 0 e desvio padrão definido como 1. 
Uma estratégia é testar as duas métricas e comparar qual se saiu melhor para o nosso problema.

MinMax
"""

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
dt_minmax = pd.DataFrame(scaler.fit_transform(dataframe), columns = dataframe.columns)
#dtTest_norm = pd.DataFrame(scaler.fit_transform(dtTest), columns = dtTest.columns)

dt_minmax

"""Standard Scaler"""

from sklearn.preprocessing import StandardScaler

scaler1 = StandardScaler()
dt_stan = pd.DataFrame(scaler1.fit_transform(dataframe), columns = dataframe.columns)

dt_stan

"""# Resultado da normalização

Para comparar as duas métricas de normalização foi utilizado o GridSearch (escolhendo o melhor k, métrica e weight) e o Cross Validation.
Para ambas métricas os parâmetros resultantes foram o mesmo.


```
{'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'distance'}
```
Ao calcular a média do resultado do Cross Validation, a métrica Standard teve uma acurácia de 85.59%, menos de 1% de diferença da métrica MinMax com 84.96% de acurácia.
Assim, optou-se por utilizar a métrica Standard.

# Treino e teste
"""

# passa para X somente os atributos preditivos, tirando a classe Prediction
#dados normalizados com standard
X = pd.DataFrame(dt_stan)

# passa para y a classe Prediction
y = pd.DataFrame(dataf["Prediction"])

"""# Distribuição de classes

Ao observar o gráfico, é possível visualizar que os emails sem spam tem mais que o dobro de emails considerados com spam. Sendo assim, precisa-se gerar um balanceamento entre as classes para melhor resultado das métricas de desempenho da KNN.
"""

class_distr = dataf.Prediction.value_counts()
print(class_distr)
class_distr.plot(kind='bar', title='Count (Prediction)')

"""# Técnicas para criar um conjunto de dados balanceados


*   **Oversampling**

Esta técnica é usada para modificar as classes de dados desiguais para criar conjuntos de dados balanceados. Quando a quantidade de dados é insuficiente, o método de sobreamostragem tenta equilibrar aumentando o tamanho de amostras raras. A técnica utilizada em oversampling é o *SMOTE*. 


> Nesta técnica, a classe minoritária é super amostrada pela produção de exemplos sintéticos em vez de pela superamostragem com substituição e para cada observação de classe minoritária, ela calcula os k vizinhos mais próximos


"""

from imblearn.over_sampling import SMOTE

smote = SMOTE(ratio='minority')
X_sm, y_sm = smote.fit_sample(X, y)

#Avaliar a acuracia do SMOTE
model = KNeighborsClassifier(n_neighbors=3,weights='distance', metric='euclidean')
scores = cross_val_score(model,X_sm, y_sm, cv = 10, scoring = 'accuracy')
print("Accuracy: %.2f%%" % (scores.mean() * 100.0))

"""Uma forma interessante de avaliar os resultados é por meio de uma matriz de confusão, que mostra as previsões correctas e incorrectas para cada classe. 


*   Na primeira linha, a primeira coluna indica quantas **classes 0 foram previstas corretamente**.
*   Na segunda coluna, quantas **classes 0 foram previstas como 1**.

*   Na segunda linha, a primeira coluna indica **quantas classes 1 foram previstas como 0**.
*   Na segunda coluna, **quantas classes 1 foram previstas corretamente.**

Portanto, quanto mais altos os valores diagonais da matriz de confusão, melhor. Assim, indicando muitas previsões corretas.
"""

X_train, X_test, y_train, y_test = train_test_split(X_sm, y_sm, test_size=0.05,random_state=42)

model = KNeighborsClassifier(n_neighbors=3)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
#print(accuracy)

#matriz de confusão para ver quanto o algoritmo acerta e erra 
conf_mat = confusion_matrix(y_true=y_test, y_pred=y_pred)
print('Confusion matrix:\n', conf_mat)

labels = ['Class 0', 'Class 1']
fig = plt.figure()
ax = fig.add_subplot(111)
cax = ax.matshow(conf_mat, cmap=plt.cm.Blues)
fig.colorbar(cax)
ax.set_xticklabels([''] + labels)
ax.set_yticklabels([''] + labels)
plt.xlabel('Predicted')
plt.ylabel('Expected')
plt.show()

"""*   **Undersampling**

Ao contrário do Oversampling, esta técnica equilibra o conjunto de dados desbalanceado, reduzindo o tamanho da classe que está em abundância.


> O método de *Tomek link * remove a sobreposição indesejada entre as classes até que todos os vizinhos mais próximos com distância mínima sejam da mesma classe. 


> Os métodos de *Cluster Centroid* substituem o cluster de amostras pelo centróide de cluster de um algoritmo K-means.

Tomek Link
"""

from imblearn.under_sampling import TomekLinks


tmlink = TomekLinks(return_indices=True, ratio='majority')
X_tkl, y_tkl, id_tkl = tmlink.fit_sample(X, y)

model = KNeighborsClassifier(n_neighbors=3,weights='distance', metric='euclidean')
scores = cross_val_score(model,X_tkl, y_tkl, cv = 10, scoring = 'accuracy')
print("Accuracy: %.2f%%" % (scores.mean() * 100.0))

X_train, X_test, y_train, y_test = train_test_split(X_tkl, y_tkl, test_size=0.05,random_state=42)

model = KNeighborsClassifier(n_neighbors=3)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
#print(accuracy)

conf_mat = confusion_matrix(y_true=y_test, y_pred=y_pred)
print('Confusion matrix:\n', conf_mat)

labels = ['Class 0', 'Class 1']
fig = plt.figure()
ax = fig.add_subplot(111)
cax = ax.matshow(conf_mat, cmap=plt.cm.Blues)
fig.colorbar(cax)
ax.set_xticklabels([''] + labels)
ax.set_yticklabels([''] + labels)
plt.xlabel('Predicted')
plt.ylabel('Expected')
plt.show()

"""Cluester Centroid"""

from imblearn.under_sampling import ClusterCentroids

cc = ClusterCentroids(ratio= 'majority')

X_cc, y_cc = cc.fit_sample(X, y)

model = KNeighborsClassifier(n_neighbors=3,weights='distance', metric='euclidean')
scores = cross_val_score(model,X_cc, y_cc, cv = 10, scoring = 'accuracy')
print("Accuracy: %.2f%%" % (scores.mean() * 100.0))

y_prediction = pd.DataFrame(y_cc,columns= ['Prediction'])
y_prediction.value_counts().plot(kind="bar")
qnt_0, qnt_1 = y_prediction.value_counts()

X_train, X_test, y_train, y_test = train_test_split(X_cc, y_cc, test_size=0.1,random_state=42)

model = KNeighborsClassifier(n_neighbors=3)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
#print(accuracy)

conf_mat = confusion_matrix(y_true=y_test, y_pred=y_pred)
print('Confusion matrix:\n', conf_mat)

labels = ['Class 0', 'Class 1']
fig = plt.figure()
ax = fig.add_subplot(111)
cax = ax.matshow(conf_mat, cmap=plt.cm.Blues)
fig.colorbar(cax)
ax.set_xticklabels([''] + labels)
ax.set_yticklabels([''] + labels)
plt.xlabel('Predicted')
plt.ylabel('Expected')
plt.show()

"""Resultado da melhor técnica de Balanceamento dos dados


> A técnica com melhor acurácia foi a de *Undersampling*, o método **Cluster Centroid com 88.03% de acurácia**.


"""

#transforma o arrat X_cc em um dataframe
Xcc = pd.DataFrame(X_cc, columns= X.columns)

dt_stan_cc = pd.concat([Xcc, y_prediction],axis=1)

dt_stan_cc

"""# Validação Cruzada (K-Fold)

Validação cruzada é uma técnica utilizada para avaliar a capacidade de generalização de um modelo a partir de um conjunto de dados. Uma das maneiras de fazer a divisão desses dados é usando o método *holdout* que consiste em dividir os dados em 70-30 de maneira aleatória, sendo esse método utilizado por nós anteriormente. 

No entanto, a desvantagem de usar essa técnica é que podemos pegar uma porção de dados de treino e teste que são parecidas, nos devolvendo uma boa avaliação do modelo nesse caso, então quando colocamos o modelo em produção com dados novos que são muito diferentes dos dados já conhecidos pelo modelo, os resultados serão péssimos. 

Outra maneira de fazer a divisão é utilizando a Validação Cruzada, que nos permite treinar e tester o modelo com todos os dados disponíveis para evitar a variância e assim obtem-se um resulta mais robusto. Sendo esse o motivo da nossa escolha por essa técnica. A sua principal desvantagem é o desempenho, então se a base de dados for muito grande a valizadação cruzada vai ser muito custosa computacionalmente.
"""

model = KNeighborsClassifier() ###
scores = cross_val_score(model, X, y, cv = 10, scoring = 'accuracy')
scores

print("Accuracy: %.2f%%" % (scores.mean() * 100.0)) #

"""# Redução de Dimensionalidade

Quando estamos trabalhando com um conjunto de dados com muitos atributos, o ideal é que seja feita uma redução de dimensionalidade. Existem várias formas de fazer isso, e neste trabalho será abordado duas formas: por extração de atributos e por seleção de atributos.

## Redução de Dimensionalidade por Extração de Atributos

### PCA (Análise de Componentes Principais)

Um dos tipos de redução de dimensionalidade por meio de extração de atributos é o PCA (Análise de Componentes Principais), que vamos aplicar a seguir.

Com o PCA, a semântica dos atributos é perdida, ou seja, as palavras em inglês não vão mais estar sendo representadas como palavras, e sim como componentes principais (P1, P2, P3, etc).

Portanto, o conjunto de dados é modificado completamente, o dataframe que antes continha os atributos/palavras e o prediction, depois de aplicado no PCA, torna-se um dataframe contendo apenas os "n" compontentes principais e o prediction.
"""

# atribui valores pro X e y pegando apenas as colunas 2 até 3002  
Xpca = pd.DataFrame(np.matrix(dataset.iloc[:,2:3003]))
ypca = pd.DataFrame(dataset.Prediction)

Xpca.head(3)

"""Aplicando o PCA"""

from sklearn.decomposition import PCA

pca = PCA(n_components=3) # número de componentes

pca_fit = pca.fit(Xpca)
pca_fit.explained_variance_ratio_ # informação para fazer o scree plot

"""Fazendo o Scree Plot"""

import matplotlib.pyplot as plt 

per_var = np.round(pca_fit.explained_variance_ratio_*100, decimals=1) # transformando para %

plt.bar(x=range(1, len(per_var)+1), height=per_var, tick_label=['PC1','PC2','PC3'])
plt.ylabel('Porcentagem da Variância Explicada')
plt.xlabel('Componentes Principais')
plt.show()

"""Projetando os Componentes"""

principalComp = pca.fit_transform(Xpca) 
principalComp # é o novo dataset

dataframeTrain = pd.DataFrame(data=principalComp, columns=['PC1', 'PC2', 'PC3']) # é o novo dataframe 
dataframeTrain = pd.concat([dataframeTrain, ypca], axis=1) # juntando com o prediction
dataframeTrain

"""Fazendo o split do teste e treino"""

Xpca = dataframeTrain.drop(["Prediction"], axis = 1) 
ypca = dataframeTrain["Prediction"]

# fazendo o split do teste e treino do novo dataset 

Xpca_train, Xpca_test, ypca_train, ypca_test = train_test_split(Xpca, ypca, test_size=0.1)

"""Aplicando o novo dataset no KNN"""

# Agora aplica o novo dataset no knn 

model = KNeighborsClassifier(n_neighbors=1, metric='euclidean')
model.fit(Xpca_train, ypca_train)

y_pred = model.predict(Xpca_test)

"""Ao aplicar o dataframe resultante do PCA no algoritmo do knn, resultou em um erro o qual não soubemos como resolver.

Medindo a acurácia
"""

acc = accuracy_score(ypca_test, y_pred) 
acc

y_pred

"""## Redução de Dimensionalidade por Seleção de Atributos

A seleção de atributos é outra maneira de fazer redução de dimensionalidade, mas em vez de modificar a estrutura do conjunto de dados como no PCA, essa técnica apenas seleciona quais são os melhores atributos do conjunto de dados, ou seja, os que mais carregam informações para descrever um problema. Ela também identifica os atributos importantes, melhorando o desempenho na classificação, além de minimizar os efeitos de ruídos e reduzir custos na coleta de dados.

Há varios métodos/algoritmos que fazem essa seleção, que serão apresentados posteriormente.

Feature Selection (Seleção de Atributos)
Por que não usamos todos os atributos no algoritmo de machine learning? 

*   Occam's Razor: por causa do princípio da Navalha de Occam, onde a melhor solução é a mais simples. Queremos que nosso modelo seja simples e explicativo, se tivermos muitos atributos, ele provavelmente ficaria muito volumoso, demorado, e difícil de ser explicado.
*   Garbage in Garbage out: princípio que diz que, literalmente, se entra lixo sai lixo, ou seja, se há atributos que não têm a necessidade de serem utilizados, como por exemplo o id que é um valor único, e se utilizado abaixaria a qualidade do resultado. Excluindo atributos que não têm muita relação com o target aumentará as chances de obter uma acurácia do resultado.
*   A seleção de atributos é tão importante quanto o modelo em si, pois se escolhermos os atributos errados, o modelo vai aprender qualquer coisa, mas se escolhermos os atributos mais adequados, mesmo um simples modelo poderia trazer bons resultados.

### Baseado em Filtros

É chamado de "baseado em filtro" porque usa a métrica selecionada para localizar atributos irrelevantes, filtrando colunas redundantes do modelo.

Utilizamos as seguintes métricas baseadas em filtro:

*   Correlação de Spearman;
*   Chi quadrado.

Correlação de Spearman
"""

from scipy.stats import pearsonr #

#selecao de atributos por correlacao #
def cor_selection(dt, prediction, nattr):
  cor_list = []
  for i in dt.columns.tolist():
    #relaciona cada atributo existente com a classe target
    #atributos que tem uma correlaçao forte com a classe, quer dizer que sao bons atributos preditivos, simples e mais usado
    cor = pearsonr(dt[i], prediction)[0]
    cor_list.append(cor)
  
  #retorna quais features foram selecionadas
  #argsort, ordena em ordem crescente
  #abs, pq pode ter correlacao negativa, correlacao postivia e negativa sao importantes
  cor_feature = dt.columns[np.argsort(np.abs(cor_list))][-nattr:].tolist()
  #cor_support, coloca true nos atributos selecionados de todos os 200
  cor_support = [True if i in cor_feature else False for i in dt.columns.tolist()]

  return cor_feature, cor_support

#retorna os atributos com maior correlacao #
#retorna os 50 atributos com maior correlacao, nattr -> 50
#cor_selected = cor_feature

cor_selected, cor_support = cor_selection(dt_stan_cc.drop(['Prediction'],axis=1), dt_stan_cc['Prediction'],500)

cor_selected #

"""Chi Quadrado"""

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from sklearn.preprocessing import MinMaxScaler

def chi_selection(dt, prediction, nattr):
  cor_list = []
  X_norm = MinMaxScaler().fit_transform(dt)

  chi_selector = SelectKBest(chi2, k=nattr)
  chi_selector.fit(X_norm, prediction)
  chi_support = chi_selector.get_support()
  chi_selected = dt.loc[:,chi_support].columns.tolist()

  return chi_selected, chi_support

chi_selected, chi_support = chi_selection(dt_stan_cc.drop(['Prediction'],axis=1), dt_stan_cc['Prediction'],500)

chi_selected

"""Info Gain (não será usado, pois sempre escolhe valores aleatórios)"""

from sklearn.feature_selection import mutual_info_classif

def mutual_info (dt, prediction, nattr):
  info_imp = mutual_info_classif(dt, prediction)

  info_selected = dt.columns[np.argsort(np.abs(info_imp))][-nattr:].tolist()
  # feature selection? 0 for not select , 1 for select
  info_support = [True if i in info_selected else False for i in dt.columns.tolist()]

  info_ranking = X.columns[np.argsort(np.abs(info_imp))]

  return info_selected, info_support

info_selected, info_support = mutual_info(dfNormalizado.drop(['Prediction'],axis=1), dfNormalizado['Prediction'],500)

info_selected

"""O método Info Gain, toda vez que é executado, sempre escolhe atributos diferentes como sendo os melhores. Acreditamos que isso atrapalharia, mesmo que minimamente, na seleção de melhores atributos, portanto, resolvemos retirá-lo.

### Baseados em Wrapper

Métodos baseados em Wrapper consideram a seleção de um conjunto de atributos como um problema de pesquisa, como por exemplo a eliminação de um atributo recursivo.

Recursive Feature Elimination (RFE)
"""

from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression

def RFE_selection(dt, prediction, nattr): # usando regressão logistica
  rfe_selector = RFE(estimator=LogisticRegression(), n_features_to_select = nattr, step=10, verbose=1) # step: iterações
  rfe_selector.fit(dt, prediction)
  rfe_support = rfe_selector.get_support()
  rfe_feature = dt.loc[:,rfe_support].columns.tolist()
  
  return rfe_feature, rfe_support

X_train
#transforma o array X_train em um dataframe
X_train_cc = pd.DataFrame(X_train, columns= X.columns)

y_train_cc = pd.DataFrame(y_train, columns= ['Prediction'])
y_train_cc

rfe_selected, rfe_support = RFE_selection(X_train_cc, y_train_cc, 500)

rfe_selected

"""### Embedded

Os métodos embedded usam algoritmos que possuem métodos de seleção de atributos embutidos neles. Por exemplo, Lasso e RF têm seus próprios métodos de seleção de recursos.
"""

from sklearn.feature_selection import SelectFromModel
from sklearn.ensemble import RandomForestClassifier

def embedded_selection(df, prediction, nattr):
  embeded_rf_selector = SelectFromModel(RandomForestClassifier(n_estimators=100), max_features=nattr) # 100 florestas, n atributos
  embeded_rf_selector.fit(df, prediction)

  embeded_rf_support = embeded_rf_selector.get_support()
  embeded_rf_feature = X.loc[:, embeded_rf_support].columns.tolist()

  print(str(len(embeded_rf_feature)), 'selected features')

  return embeded_rf_feature, embeded_rf_support

embedded_selected, embedded_support = embedded_selection(dt_stan_cc.drop(['Prediction'],axis=1), dt_stan_cc['Prediction'],500)

embedded_selected

"""## Comparação entre os Métodos de Seleção de Atributos"""

feature_selection_results = pd.DataFrame({'Feature': X.columns.tolist(), 'Pearson' : cor_support, 'Chi' : chi_support, #
                                           'RFE' : rfe_support, 'Embedded' : embedded_support})

feature_selection_results['Total'] = np.sum(feature_selection_results, axis=1)

feature_selection_results = feature_selection_results.sort_values(['Total', 'Feature'], ascending=False)

feature_selection_results.head(110)

"""Inicialmente, tínhamos feito a seleção de atributos no dataset original (sem ter sido normalizado e dividido em treino e teste), utilizando 3 métodos (Spearman, Chi quadrado e RFE). Comparando esses 3 métodos, apenas 65 atributos obtiveram a nota máxima (3).

Posteriormente, adicionamos outro método Embedded, totalizando 4 métodos, e testamos a seleção de atributos no dataset já normalizado. Feito isso, foi possível obter 103 atributos com nota máxima (4). Acredita-se que a normalização melhorou o desempenho da seleção de atributos em cada método.

### Novo Dataset com os melhores atributos
"""

# guarda todos atributos que tem o total == 4 #
best = feature_selection_results.loc[feature_selection_results['Total'] == 4] 

# transforma a coluna 'feature' em lista 
best_feat = best['Feature'].tolist() 

# transforma os atributos do df original em uma lista
features_orign = X.columns.tolist()

#dataset original transposto 
transp_orign = dt_stan_cc.T 

# nomeando a coluna index para conseguir manipular ela
transp_orign.index.name = 'features'

# lista dos melhores atributos vira um dataframe e nomeia a coluna index
best_feat = pd.DataFrame(data = best_feat, columns=['features'])
best_feat.set_index('features')

# merge do df original com o df melhores atributos, ele deixa só os comuns
# aqui as features são a segunda coluna
best_attributes = pd.merge(transp_orign, best_feat, how='inner', on = 'features') 

# transforma as features em coluna index antes de transpor
best_attributes = best_attributes.set_index('features')

new_dataframe = best_attributes.T
new_dataframe.head(3)

new = pd.concat([new_dataframe, y_prediction],axis=1) #
new.head(3)

"""X e y do novo dataset """

# fazendo o split do teste e treino do novo dataset #
# criando outras variáveis para representar o X e o y, para não misturar com a análise de outra parte do trabalho 
Xnew = new.drop(['Prediction'], axis=1)
ynew = new['Prediction']

Xnew_train, Xnew_test, ynew_train, ynew_test = train_test_split(Xnew, ynew, test_size=0.1)

"""# Descobrindo o melhor valor de k

Escolher o valor de k é importante, pois um k muito baixo pode não ser suficiente, mas um k muito grande é computacionalmente caro. Como o número de classes do nosso modelo é dois (par), escolhemos o k como sendo ímpar para evitar confusão na classificação entre as duas classes, ou seja, para que não haja empate entre a escolha de uma classe.

Existem várias maneiras de descobrir o melhor valor de k. Nós testamos três maneiras, que serão apresentadas a seguir.

## Por raiz quadrada

Caso o valor de k retorne um número par, faz-se a subtração do mesmo por 1.
"""

import math 
math.sqrt(len(ynew_train))

"""## Por Grid Search"""

#dados normalizados com standard #
data = pd.DataFrame(new.drop(["Prediction"], axis=1))
target = pd.DataFrame(new["Prediction"])

# regressão linear
pipeline = Pipeline(
    [
     ('selector',SelectKBest(f_regression)),
     ('model',KNeighborsClassifier())
    ]
)

# grid search
search = GridSearchCV(
    estimator = pipeline,
    param_grid = {'selector__k':[3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,79,81]},
    n_jobs=-1,
    scoring="neg_mean_squared_error",
    cv=5,
    verbose=3
)

search.fit(data,target)

search.best_score_

search.best_params_
# resultado: k = 5

"""## Outro algoritmo procurando o melhor k"""

#k=8 -> metrica 'euclidean'
nkf = 5 #number of folds ?
vk = [] # armazena os valores de k
vscore = []
for k in range(1, 49, 2):
    model = KNeighborsClassifier(n_neighbors=k,metric = 'euclidean')
    # realiza a validação cruzada
    cv = cross_validate(model, Xnew_train, np.ravel(ynew_train,order='C'), cv=nkf)
    #print('k:', k, 'accurace:', cv['test_score'].mean())
    vscore.append(cv['test_score'].mean()) 
    vk.append(k)

plt.figure(figsize=(6,4))
plt.plot(vk, vscore, '-bo')
plt.xlabel('k', fontsize = 15)
plt.ylabel('Acuracy', fontsize = 15)
plt.show(True)

best_k = np.argmax(vscore)+1
print('Melhor k:', best_k)

"""# Descobrindo a melhor métrica"""

from sklearn.model_selection import train_test_split

#dados normalizados com standard
X = pd.DataFrame(dt_stan)
y = pd.DataFrame(dataf["Prediction"])
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)

knn = KNeighborsClassifier()
knn.fit(X_train, y_train)

print('Train Score', knn.score(X_train, y_train))
print('Test Score', knn.score(X_test, y_test))

knn.predict(X_test)[:9]
knn.predict_proba(X_test)[:9]

# Usando GridSearch
from sklearn.model_selection import GridSearchCV

grid_params = {
    #'n_neighbors': [3,5,11,19,21,23,27,29,31,33,35,37,39,41,53,55,59,61,63,65]
    'weights': ['uniform', 'distance'],
    'metric': ['euclidean', 'manhattan', 'minkowski']
}

gs = GridSearchCV(
    KNeighborsClassifier(n_neighbors=3),
    grid_params,
    verbose = 1,
    cv = 5,
    n_jobs = -1
)
gs_results = gs.fit(X_train, y_train)

gs_results.best_score_
gs_results.best_estimator_
gs_results.best_params_
#{'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'distance'} -> standadrd

"""A melhor métrica foi a Euclidiana.

# KNN

Dentre as 3 técnicas de seleção do melhor k apresentadas, cada uma delas gerou um k diferente. Para escolher qual técnica utilizar, aplicamos o knn para cada um dos três valores de k, a fim de descobrir qual delas resulta na melhor acurácia.
"""

from sklearn.neighbors import KNeighborsClassifier 
from sklearn.metrics import accuracy_score
import sklearn

xtest = Xnew_test
xtrain = Xnew_train

model = sklearn.neighbors.KNeighborsClassifier(n_neighbors=5,metric = 'euclidean') 
model.fit(xtrain, ynew_train)
y_pred = model.predict(xtest)

acc = accuracy_score(ynew_test, y_pred)
print(acc)

"""Optamos por utilizar o k = 5 como foi recomendado pelo Grid Search.

# Comparando as técnicas de Redução de Dimensionalidade

Para comparar qual das duas técnicas de redução de dimensionalidade obteve o melhor desempenho, utilizamos novamente a validação cruzada.

## Acurácia da Seleção de Atributos

Aplicando o dataframe o qual utilizou-se da seleção de atributos na validação cruzada, obteve-se o seguinte resultado.
"""

model = KNeighborsClassifier(n_neighbors=5) #
scores = cross_val_score(model, Xnew, ynew, cv = 10, scoring = 'accuracy')
scores

print("Accuracy: %.2f%%" % (scores.mean() * 100.0)) #

"""## Acurácia da Extração de Atributos (PCA)

Aplicando o dataframe o qual utilizou-se da extração de atributos (PCA) na validação cruzada, obteve-se o seguinte resultado.
"""

model = KNeighborsClassifier(n_neighbors=5)
scores = cross_val_score(model, Xpca, ypca, cv = 10, scoring = 'accuracy')
scores

print("Accuracy: %.2f%%" % (scores.mean() * 100.0))

"""Matriz Confusão do dataframe construído a partir dos métodos de seleção de atributo


"""

#matriz de confusão para ver quanto o algoritmo acerta e erra 
conf_mat = confusion_matrix(y_true=ynew_test, y_pred=y_pred)
print('Confusion matrix:\n', conf_mat)

labels = ['Class 0', 'Class 1']
fig = plt.figure()
ax = fig.add_subplot(111)
cax = ax.matshow(conf_mat, cmap=plt.cm.Blues)
fig.colorbar(cax)
ax.set_xticklabels([''] + labels)
ax.set_yticklabels([''] + labels)
plt.xlabel('Predicted')
plt.ylabel('Expected')
plt.show()